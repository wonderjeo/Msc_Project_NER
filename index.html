<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-05-21 Tue 13:56 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Named Entity and Rare Word Recognition</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wang Dezhao, Liu Wenjing, Wei Jian" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="style.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Named Entity and Rare Word Recognition</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgc557a4a">1. Schedule</a>
<ul>
<li><a href="#orgeb98f0b">1.1. <span class="done DONE">DONE</span> Small labeling(200~)</a></li>
<li><a href="#orgc273a62">1.2. <span class="todo TODO">TODO</span> Literature Review(baidu)</a></li>
<li><a href="#org3559733">1.3. <span class="todo TODO">TODO</span> WebPage</a></li>
<li><a href="#org39802c2">1.4. <span class="todo TODO">TODO</span> Standford CoreNLP test accuracy</a></li>
<li><a href="#org2a6249e">1.5. <span class="todo TODO">TODO</span> Labeling</a></li>
<li><a href="#orgee119ac">1.6. <span class="todo TODO">TODO</span> Revise the existing model(bi-LSTM+CRF)</a></li>
<li><a href="#orgbf5dbf6">1.7. <span class="todo TODO">TODO</span> Rare word recognition</a></li>
</ul>
</li>
<li><a href="#org068f038">2. Literature Review</a>
<ul>
<li><a href="#org02f90d7">2.1. Neural Architectures for NER(Guillaume)</a>
<ul>
<li><a href="#org84a2404">2.1.1. Model</a></li>
<li><a href="#orga026019">2.1.2. LSTM</a></li>
<li><a href="#org2cc91f1">2.1.3. CRF</a></li>
<li><a href="#org782a598">2.1.4. Parameterization and Training</a></li>
<li><a href="#org36d8cca">2.1.5. Tagging Schemes</a></li>
<li><a href="#org275d129">2.1.6. Input Word Embeddings</a></li>
<li><a href="#orgf91c6c0">2.1.7. Experiments</a></li>
</ul>
</li>
<li><a href="#org8977619">2.2. Bidirectional LSTM-CRF Models for Sequence Tagging(Baidu research)</a>
<ul>
<li><a href="#org3168ed4">2.2.1. Model</a></li>
<li><a href="#org3d2c780">2.2.2. LSTM</a></li>
<li><a href="#org29b19bc">2.2.3. Bidirectional LSTM Networks</a></li>
<li><a href="#org0e13d67">2.2.4. CRF Networks</a></li>
<li><a href="#orgb091cc7">2.2.5. LSTM-CRF Networks</a></li>
<li><a href="#org6ce727a">2.2.6. Training procedure</a></li>
<li><a href="#org121d890">2.2.7. Experiments</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgc557a4a" class="outline-2">
<h2 id="orgc557a4a"><span class="section-number-2">1</span> Schedule</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orgeb98f0b" class="outline-3">
<h3 id="orgeb98f0b"><span class="section-number-3">1.1</span> <span class="done DONE">DONE</span> Small labeling(200~)</h3>
</div>
<div id="outline-container-orgc273a62" class="outline-3">
<h3 id="orgc273a62"><span class="section-number-3">1.2</span> <span class="todo TODO">TODO</span> Literature Review(baidu)</h3>
</div>
<div id="outline-container-org3559733" class="outline-3">
<h3 id="org3559733"><span class="section-number-3">1.3</span> <span class="todo TODO">TODO</span> WebPage</h3>
</div>
<div id="outline-container-org39802c2" class="outline-3">
<h3 id="org39802c2"><span class="section-number-3">1.4</span> <span class="todo TODO">TODO</span> Standford CoreNLP test accuracy</h3>
</div>
<div id="outline-container-org2a6249e" class="outline-3">
<h3 id="org2a6249e"><span class="section-number-3">1.5</span> <span class="todo TODO">TODO</span> Labeling</h3>
</div>
<div id="outline-container-orgee119ac" class="outline-3">
<h3 id="orgee119ac"><span class="section-number-3">1.6</span> <span class="todo TODO">TODO</span> Revise the existing model(bi-LSTM+CRF)</h3>
</div>
<div id="outline-container-orgbf5dbf6" class="outline-3">
<h3 id="orgbf5dbf6"><span class="section-number-3">1.7</span> <span class="todo TODO">TODO</span> Rare word recognition</h3>
</div>
</div>

<div id="outline-container-org068f038" class="outline-2">
<h2 id="org068f038"><span class="section-number-2">2</span> Literature Review</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org02f90d7" class="outline-3">
<h3 id="org02f90d7"><span class="section-number-3">2.1</span> Neural Architectures for NER(Guillaume)</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="outline-container-org84a2404" class="outline-4">
<h4 id="org84a2404"><span class="section-number-4">2.1.1</span> Model</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li>bidirectional LSTMs and CRF(*better)</li>
<li>Stack LSTM</li>
<li>the paper said the model learn from 2 source : supervised and unsupervised corpus</li>
<li>the model are designed to capture 2 intuitions
<ol class="org-ol">
<li>names often consist of multiple tokens, reasoning jointly over tagging decisions for each token is important</li>
<li>token-level evidence for "being a name" includes both orthographic evidence(what does the word being tagged as a name look like?)(use character-based word representation model) and distributional evidence(where does the word being tagged tend to occur in a corpus?) combine these representations with distributional representations</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orga026019" class="outline-4">
<h4 id="orga026019"><span class="section-number-4">2.1.2</span> LSTM</h4>
<div class="outline-text-4" id="text-2-1-2">
<ul class="org-ul">
<li>input: a given sentence (\(x_1, x_2, ..., x_n\))</li>
<li>\(x_n\) is represented as a d-dimensional vector</li>
<li>an LSTM(forward LSTM) computes a representation \(\vec{h_t}\) of the left context of the sentence at every word \(t\).</li>
<li>a second LSTM(backward LSTM) that reads the same sequence in reverse computes the right context \(\stackrel{\leftarrow}{h_t}\)</li>
<li>the representation of a word using this model is obtained by concatenating its left and right context representations, \(h_t = [\vec{h_t};\stackrel{\leftarrow}{h_t}]\).</li>
</ul>
</div>
</div>

<div id="outline-container-org2cc91f1" class="outline-4">
<h4 id="org2cc91f1"><span class="section-number-4">2.1.3</span> CRF</h4>
<div class="outline-text-4" id="text-2-1-3">
<ul class="org-ul">
<li>the grammer that characterizes interpretable sequences of tags imposes several hard constraints(e.g., I-PER cannot follow B-LOC) that would be impossible to model with independence assumptions.</li>
<li>for an input sentence</li>
</ul>
\begin{equation} 
X = (x_1, x_2, ..., x_n) 
\end{equation} 
<ul class="org-ul">
<li>consider \(P\) to be the matrix of scores output by the bidirectional LSTM network. \(P\) is of size \(n \times k\), where \(k\) is the number of distinct jutags, and \(P_i,j\) corresponds to the score of the \(j^{th}\) tag of the \(i^{th}\) word in a sentence.</li>
<li>for a sequence of predictions</li>
</ul>
\begin{equation}
y = (y_1, y_2, ..., y_n)
\end{equation}
<ul class="org-ul">
<li>we define its score to be</li>
</ul>
\begin{equation}
s(X, y) = \sum_{i=0}^{n}A_{{y_i},{y_{i+1}}} + \sum_{i=1}^{n}P_{i,y_i}
\end{equation}
<ul class="org-ul">
<li>\(A\) is a matrix of transition scores such that \(A_{i,j}\) represents the score of a transition from the tag \(i\) to tag \(j\). &amp;y<sub>0</sub>&amp; and \(y_n\) are the \(start\) and \(end\) tags of a sentence, that we add to the set of possible tags.</li>
<li>\(A\) is therefore a square matrix of size \(k+2\)</li>
<li>a softmax over all possible tag sequences yields a probability for the sequence \(y\):</li>
</ul>
\begin{equation}
p(y|X) = \frac{e^{s(X,y)}}{\sum_{\widetilde{y}\in{Y_X}}e^{s(X,\widetilde{y})}}
\end{equation}
<ul class="org-ul">
<li>during training, we maximizie the log-probability if the correct tag sequence</li>
</ul>
</div>
</div>

<div id="outline-container-org782a598" class="outline-4">
<h4 id="org782a598"><span class="section-number-4">2.1.4</span> Parameterization and Training</h4>
<div class="outline-text-4" id="text-2-1-4">
<ul class="org-ul">
<li>the scores associated with each tagging decision for each token (i.e., the \(P_(i,y)\)'s) are defined to be the dot product between the embedding of a word-in-context computed with a bidirectional LSTM</li>
<li>the parameters of this model are thus the matrix of bigram compatibility scores \(A\), and the parameters that give rise to the matrix \(P\), namely the parameters of the bidirectional LSTM, the linear feature weights, and the word embeddings.</li>
<li>the sequence of word embeddings is given as input to a bidirectional LSTM, which returns a representation of the left and right context for each word</li>
<li>these representations are concatenated(\(c_i\)) and linearly projected onto a layer whose size is equal to the number of distinct tags.</li>
<li>add a hidden layer between \(c_i\) and the CRF layer marginally improved the results</li>
</ul>
</div>
</div>

<div id="outline-container-org36d8cca" class="outline-4">
<h4 id="org36d8cca"><span class="section-number-4">2.1.5</span> Tagging Schemes</h4>
<div class="outline-text-4" id="text-2-1-5">
<ul class="org-ul">
<li>the paper use IOBES tagging scheme, because tagging a word as I-label with high-confidence narrows down the choices for the subsequent word to I-label or E-label</li>
<li>however, there is no siginificant improvement over the IOB tagging scheme</li>
</ul>
</div>
</div>

<div id="outline-container-org275d129" class="outline-4">
<h4 id="org275d129"><span class="section-number-4">2.1.6</span> Input Word Embeddings</h4>
<div class="outline-text-4" id="text-2-1-6">
<ul class="org-ul">
<li>the input layers are vector representations of individual words</li>
<li>Since many languages have orthographic or morphological evidence that something</li>
<li>Character-based models of words
<ul class="org-ul">
<li>learn character-level feature while training. Learning character-level embeddings has the advantage of learning representations specific to the task and domain at hand. They have been found useful for morphologically rich languages and to handle the out-of-vocabulary problem for tasks like part-of-speech tagging and language modeling or dependency parsing.</li>
<li>generate a word embedding for a word from its characters. A character lookup table initialized at random contains an embedding for every character. The character embeddings corresponding to every character in a word are given in direct and reverse order to a forward and a backward LSTM. The embedding for a word derived from its characters is the concatenation of its forward and backward representations from the bidirectional LSTM.</li>
<li>this character-level representation is then concatenated with a word-level representation from a word lookup-table.</li>
</ul></li>
<li>Pretrained embedding
<ul class="org-ul">
<li>use pretrained word embeddings to initialized the lookup table. There is a significant improvements using pretrained word embeddings over randomly initialized ones. Embeddings are pretrained using skip-n-gram, a variation of word2vec that accounts for word order. These embeddings are fine-tuned during training.</li>
<li>word embeddigs for English are trained using the English Gigaword version 4(with the LA Times and NY Times portions removed).</li>
<li>use an embedding dimension of 100 for English</li>
</ul></li>
<li>Dropout training
<ul class="org-ul">
<li>to encourage the model to depend on both representations, the paper uses dropout training, applying a dropout mask to the final embedding layer just before the input to the bidirectional LSTM. There is a significant improvement in the model's performance after using dropout.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf91c6c0" class="outline-4">
<h4 id="orgf91c6c0"><span class="section-number-4">2.1.7</span> Experiments</h4>
<div class="outline-text-4" id="text-2-1-7">
<hr />
</div>
</div>
</div>
<div id="outline-container-org8977619" class="outline-3">
<h3 id="org8977619"><span class="section-number-3">2.2</span> Bidirectional LSTM-CRF Models for Sequence Tagging(Baidu research)</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<div id="outline-container-org3168ed4" class="outline-4">
<h4 id="org3168ed4"><span class="section-number-4">2.2.1</span> Model</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li>LSTM</li>
<li>BI-LSTM</li>
<li>LSTM-CRF</li>
<li>BI-LSTM-CRF</li>
</ul>
</div>
</div>
<div id="outline-container-org3d2c780" class="outline-4">
<h4 id="org3d2c780"><span class="section-number-4">2.2.2</span> LSTM</h4>
<div class="outline-text-4" id="text-2-2-2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">LSTM Cell</th>
<th scope="col" class="org-left">Function</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">input gate(i<sub>t</sub>)</td>
<td class="org-left">\(\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i)\)</td>
</tr>

<tr>
<td class="org-left">forget gate(f<sub>t</sub>)</td>
<td class="org-left">\(\sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1} + b_f)\)</td>
</tr>

<tr>
<td class="org-left">output gate(o<sub>t</sub>)</td>
<td class="org-left">\(\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_{t} + b_o)\)</td>
</tr>

<tr>
<td class="org-left">cell vector(c<sub>t</sub>)</td>
<td class="org-left">\(f_{t}c_{t-1} + i_{t}tanh(W_{xc}x_t + W_{hc}h_(t-1) +b_c)\)</td>
</tr>

<tr>
<td class="org-left">h<sub>t</sub></td>
<td class="org-left">\(o_{t}tanh(c_t)\)</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org29b19bc" class="outline-4">
<h4 id="org29b19bc"><span class="section-number-4">2.2.3</span> Bidirectional LSTM Networks</h4>
<div class="outline-text-4" id="text-2-2-3">
<ul class="org-ul">
<li>using a bidirectional LSTM network can effiently make use of past features and future features for specific time frame</li>
<li>train bidirectional LSTM using backpropagation through time (BPTT)</li>
</ul>
</div>
</div>

<div id="outline-container-org0e13d67" class="outline-4">
<h4 id="org0e13d67"><span class="section-number-4">2.2.4</span> CRF Networks</h4>
<div class="outline-text-4" id="text-2-2-4">
<ul class="org-ul">
<li>there are 2 ways to make use of neighbor tag information in predicting current tags
<ul class="org-ul">
<li>the first is to predict a distribution of tags for each time step and then use beam-like decoding to find optimal tag sequences</li>
<li>the second one is to focus on sentence level instead of individual positioins, thus leading to Conditional Random Fields (CRF) models</li>
</ul></li>
<li>the inputs and outputs are directly connected</li>
</ul>
</div>
</div>

<div id="outline-container-orgb091cc7" class="outline-4">
<h4 id="orgb091cc7"><span class="section-number-4">2.2.5</span> LSTM-CRF Networks</h4>
<div class="outline-text-4" id="text-2-2-5">
<ul class="org-ul">
<li>the network can efficiently use past input features via a LSTM layer and sentence level tag information via a CRF layer</li>
<li>a CRF layer is represented by lines which connect-consecutive output layers</li>
<li>With such a layer, we can efficiently use past and future tags to predict the current tag, which is similar to the use of past and future input features via a bidirectional LSTM network</li>
</ul>
</div>
</div>

<div id="outline-container-org6ce727a" class="outline-4">
<h4 id="org6ce727a"><span class="section-number-4">2.2.6</span> Training procedure</h4>
<div class="outline-text-4" id="text-2-2-6">
<ul class="org-ul">
<li>in each epoch, divide the whole training data to batches and process one batch at a time</li>
<li>for each batch, first run bidirectional LSTM-CRF model forward pass which includes the forward pass for both forward state and backward state of LSTM. As a result, we get the output score for all tags at all positions</li>
<li>then run CRF layer forward and backward pass to compute gradients for network output and state transition edges</li>
<li>after that, back propogate the errors from the output to the input, which includes the backward pass for both forward and backward states of LSTM</li>
<li>finally, update the network parameters which include the state transition matrix \(A\), and the original bidirectional LSTM parameters \(\theta\)</li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">step</th>
<th scope="col" class="org-left">training procedure</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">bidirectional LSTM-CRF model forward pass: forward pass for forward state LSTM and forward pass for backward state LSTM</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">CRF layer forward and backward pass</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-left">bidirectional LSTM-CRF model backward pass: backward pass for forward state LSTM and backward pass for backward state LSTM</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-left">update parameter</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org121d890" class="outline-4">
<h4 id="org121d890"><span class="section-number-4">2.2.7</span> Experiments</h4>
<div class="outline-text-4" id="text-2-2-7">
<ul class="org-ul">
<li>Word embedding
<ul class="org-ul">
<li>it has been shown in that word embedding plays a vital role to improve sequence tagging performance</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Wang Dezhao, Liu Wenjing, Wei Jian</p>
<p class="date">Created: 2019-05-21 Tue 13:56</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
